# -*- coding: utf-8 -*-
"""ONNX.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aHZkSj3kGa0_L_uahY_z71PIsPegsjxt
"""

# Commented out IPython magic to ensure Python compatibility.
# importing libraries
# %matplotlib inline
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.signal import find_peaks
from scipy.stats import entropy
#import warnings
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from scipy.stats import pearsonr

#warnings.filterwarnings('ignore')
mypath1 = '/content/drive/MyDrive/WISDM_ar_v1.1_raw.txt'
mypath = '/content/drive/MyDrive/time_series_data_human_activities.csv'

# Open the file for reading
with open('/content/drive/MyDrive/WISDM_ar_v1.1_raw.txt', 'r') as file:
    lines = file.readlines()

processed_list = []

for i, line in enumerate(lines):
    try:
        # Split the line by comma
        parts = line.strip().split(',')

        # Check if there are at least 6 parts
        if len(parts) >= 6:
            # Split the last part by semicolon and take the first part
            last_part = parts[5].split(';')[0].strip()

            # Check if the last part is not empty
            if last_part:
                # Create a new list with the desired elements
                temp = [parts[0], parts[1], parts[2], parts[3], parts[4], last_part]
                processed_list.append(temp)
        else:
            print('Error at line number:', i, '- Not enough elements')
    except Exception as e:
        print('Error at line number:', i, '-', str(e))

#har_df = pd.read_csv(mypath)

har_df = pd.DataFrame(processed_list)
# Add the desired column names to the DataFrame
har_df.columns = ['user', 'activity', 'timestamp', 'x-axis', 'y-axis', 'z-axis']
# Convert 'x-axis,' 'y-axis,' and 'z-axis' columns to float
har_df[['x-axis', 'y-axis', 'z-axis']] = har_df[['x-axis', 'y-axis', 'z-axis']].astype(float)

# removing null values
har_df = har_df.dropna()
har_df.shape

# drop the rows where timestamp is 0
har_df = har_df[har_df['timestamp'] != 0]
# Drop rows where 'timestamp' is 0 and 'x-axis' is 0
har_df = har_df[har_df['x-axis'] != 0]
har_df = har_df[har_df['y-axis'] != 0]
har_df = har_df[har_df['z-axis'] != 0]



# now arrange data in ascending order of the user and timestamp
har_df = har_df.sort_values(by = ['user', 'timestamp'], ignore_index=True)

# Assuming you have a DataFrame named har_df with the given columns
# Define the mapping for label consolidation
label_mapping = {
    'Upstairs': 'Stairs',
    'Downstairs': 'Stairs'
}

# Apply the label consolidation
har_df['activity'] = har_df['activity'].map(label_mapping).fillna(har_df['activity'])

# Print the first few rows of the modified DataFrame
#har_df['activity']
df = har_df

# Calculate magnitude of acceleration and add it as a new column
df['magnitude'] = np.sqrt(df['x-axis']**2 + df['y-axis']**2 + df['z-axis']**2)

df

x_list = []
y_list = []
z_list = []
m_list = []
labels = []

window_size = 100
step_size = 50

for i in range(0, df.shape[0] - window_size, step_size):
    xs = df['x-axis'].values[i: i + 100]
    ys = df['y-axis'].values[i: i + 100]
    zs = df['z-axis'].values[i: i + 100]
    ms = df['magnitude'].values[i: i + 100]
    unique_values, counts = np.unique(df['activity'].values[i: i + 100], return_counts=True)
    label = unique_values[np.argmax(counts)]


    x_list.append(xs)
    y_list.append(ys)
    z_list.append(zs)
    m_list.append(ms)
    labels.append(label)

import numpy as np

# Check for NaN values in x_list
nan_values_x = np.isnan(x_list)

# Check for NaN values in y_list
nan_values_y = np.isnan(y_list)

# Check for NaN values in z_list
nan_values_z = np.isnan(z_list)

# Check for NaN values in m_list
nan_values_m = np.isnan(m_list)

# Count the number of NaN values in each list
count_nan_x = np.sum(nan_values_x)
count_nan_y = np.sum(nan_values_y)
count_nan_z = np.sum(nan_values_z)
count_nan_m = np.sum(nan_values_m)

print("Number of NaN values in x_list:", count_nan_x)
print("Number of NaN values in y_list:", count_nan_y)
print("Number of NaN values in z_list:", count_nan_z)
print("Number of NaN values in m_list:", count_nan_m)

# converting the signals from time domain to frequency domain using FFT
x_list_fft = pd.Series(x_list).apply(lambda x: np.abs(np.fft.fft(x))[0:5])
y_list_fft = pd.Series(y_list).apply(lambda x: np.abs(np.fft.fft(x))[0:5])
z_list_fft = pd.Series(z_list).apply(lambda x: np.abs(np.fft.fft(x))[0:5])
m_list_fft = pd.Series(m_list).apply(lambda x: np.abs(np.fft.fft(x))[0:5])

X_Data = pd.DataFrame()

# Calculate the total sum for 'y_list'
y_list_total_sum_abs = pd.Series(y_list).apply(lambda y: np.sum(np.abs(y)))

X_Data['y_total_sum_abs'] = y_list_total_sum_abs

# energy
X_Data['x_energy'] = pd.Series(x_list).apply(lambda x: np.sum(np.abs(x)**2))
X_Data['y_energy'] = pd.Series(x_list).apply(lambda x: np.sum(np.abs(x)**2))
X_Data['z_energy'] = pd.Series(y_list).apply(lambda x: np.sum(np.abs(x)**2))

# Define a function to calculate the autocorrelation for a given segment
def calculate_autocorrelation(segment):
     #autocorrelation = np.correlate(segment, segment, mode='full')
     #correlation_coefficients = autocorrelation / np.max(autocorrelation)
     #return correlation_coefficients

    mu = segment.mean()

    autocorrelation = []

    for lag in range(0, 9):
        segment_series = pd.Series(segment)
        numerator = sum((segment_series - mu).iloc[lag:] * (segment_series.shift(lag) - mu).iloc[lag:])
        denominator = sum((segment_series - mu) ** 2)
        acf = numerator / denominator
        autocorrelation.append(acf)

    return np.array(autocorrelation)



# Assuming x_list, y_list, z_list, and m_list are your input data lists

# Calculate autocorrelation for 'z_list'
az_list = pd.Series(z_list).apply(lambda z: calculate_autocorrelation(z))

# Calculate autocorrelation for 'm_list'
am_list = pd.Series(m_list).apply(lambda m: calculate_autocorrelation(m))

am_list[0]

# Define a function to calculate skewness with limited precision
def safe_skew(x):
    return stats.skew(x, axis=0, bias=False)


# Define a function to calculate kurtosis with limited precision
def safe_kurtosis(x):
    return stats.kurtosis(x, axis=0, fisher=False, bias=False)


def calculate_rms(series):
    return np.sqrt(np.mean(series ** 2))

# mean
stats_columns = {
    'y_mean':pd.Series(y_list).apply(lambda x: x.mean()),
    'am_mean':pd.Series(am_list).apply(lambda x: x.mean()),

    #variance
    'y_variance':pd.Series(y_list).apply(lambda x: x.var()),
    'm_variance':pd.Series(m_list).apply(lambda x: x.var()),
    'am_variance':pd.Series(m_list).apply(lambda x: x.var()),

    # std dev
    'x_std':pd.Series(x_list).apply(lambda x: x.std()),
    'y_std':pd.Series(y_list).apply(lambda x: x.std()),
    'z_std':pd.Series(z_list).apply(lambda x: x.std()),
    'm_std':pd.Series(m_list).apply(lambda x: x.std()),

    # avg absolute diff
    'y_aad':pd.Series(y_list).apply(lambda x: np.mean(np.absolute(x - np.mean(x)))),
    'z_aad':pd.Series(z_list).apply(lambda x: np.mean(np.absolute(x - np.mean(x)))),
    'm_aad':pd.Series(m_list).apply(lambda x: np.mean(np.absolute(x - np.mean(x)))),
    # median

    'y_median':pd.Series(y_list).apply(lambda x: np.median(x)),

    # median abs dev
    'x_mad':pd.Series(x_list).apply(lambda x: np.median(np.absolute(x - np.median(x)))),
    'm_mad':pd.Series(m_list).apply(lambda x: np.median(np.absolute(x - np.median(x)))),

    # interquartile range
    'm_IQR':pd.Series(m_list).apply(lambda x: np.percentile(x, 75) - np.percentile(x, 25)),

    # skewness
    'az_skewness':pd.Series(az_list).apply(lambda x: stats.skew(x)),

    # kurtosis
    'az_kurtosis':pd.Series(az_list).apply(lambda x: stats.kurtosis(x)),
    'x_rms':pd.Series(x_list).apply(lambda x: calculate_rms(x)),
    'y_rms':pd.Series(y_list).apply(lambda x: calculate_rms(x)),

}




stats_fft_columns = {
    'x_mean_fft':pd.Series(x_list_fft).apply(lambda x: x.mean()),
    'm_mean_fft':pd.Series(m_list_fft).apply(lambda x: x.mean()),

     # FFT std dev
    'z_std_fft':pd.Series(z_list_fft).apply(lambda x: x.std()),
      # FFT avg absolute diff
    'x_aad_fft':pd.Series(x_list_fft).apply(lambda x: np.mean(np.absolute(x - np.mean(x)))),
     # avg absolute diff
    'x_mad_fft':pd.Series(x_list_fft).apply(lambda x: np.mean(np.absolute(x - np.mean(x)))),
         # FFT energy
    'x_energy_fft' : pd.Series(x_list_fft).apply(lambda x: np.sum(np.abs(x)**2)),
    'y_energy_fft' : pd.Series(y_list_fft).apply(lambda x: np.sum(np.abs(x)**2)),
    'z_energy_fft' : pd.Series(z_list_fft).apply(lambda x: np.sum(np.abs(x)**2)),
    'x_rms_fft':pd.Series(x_list_fft).apply(lambda x: calculate_rms(x)),
    'y_rms_fft':pd.Series(y_list_fft).apply(lambda x: calculate_rms(x)),
    'z_rms_fft':pd.Series(z_list_fft).apply(lambda x: calculate_rms(x)),

}



X_Data = pd.concat([X_Data, pd.DataFrame(stats_columns), pd.DataFrame(stats_fft_columns)], axis=1)

# FFT min
x_min_fft = pd.Series(x_list_fft).apply(lambda x: x.min())
m_min_fft = pd.Series(m_list_fft).apply(lambda x: x.min())

# FFT max
x_max_fft = pd.Series(x_list_fft).apply(lambda x: x.max())
X_Data['x_max_fft'] = x_max_fft
X_Data['y_max_fft'] = pd.Series(y_list_fft).apply(lambda x: x.max())
X_Data['z_max_fft'] = pd.Series(z_list_fft).apply(lambda x: x.max())
m_max_fft = pd.Series(m_list_fft).apply(lambda x: x.max())

# FFT max-min diff
X_Data['x_maxmin_diff_fft'] = x_max_fft - x_min_fft
X_Data['m_maxmin_diff_fft'] = m_max_fft - m_min_fft

X_Data

# Split the data into training (80%) and temporary data (20%)
X_train, X_test, y_train, y_test = train_test_split(X_Data, labels, test_size=0.2, random_state=42)

X_test

y_test

# Calculate class weights
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)

# Create a dictionary mapping class labels to their respective class weights
class_weight_dict = {class_label: weight for class_label, weight in zip(np.unique(y_train), class_weights)}

# Get the best parameters and best accuracy
best_params = 10.0

# Print the results
print("Best Parameters:", best_params)

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer

numeric_features = list(range(X_train.shape[1]))

classifier = SVC(C=10.0, kernel='linear', class_weight=class_weight_dict)


numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features)
    ])

model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', classifier)
])

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.metrics import sensitivity_score, specificity_score
#from sklearn.metrics import plot_confusion_matrix
import matplotlib.pyplot as plt


model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Generate a classification report
report = classification_report(y_test, y_pred)
print("Classification Report:\n", report)

# Calculate sensitivity and specificity
sensitivity = sensitivity_score(y_test, y_pred, average='micro')  # You can change the 'average' parameter as needed
specificity = specificity_score(y_test, y_pred, average='micro')  # You can change the 'average' parameter as needed

# Compute and print accuracy score
accuracy = accuracy_score(y_test, y_pred)
print('Model accuracy: {0:0.4f}'.format(accuracy))
print(f'Model Sensitivity: {sensitivity:0.4f}')
print(f'Model Specificity: {specificity:0.4f}')

from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
import seaborn as sns
import matplotlib.pyplot as plt




# create confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Define your class labels
class_labels = ['Jogging', 'Sitting', 'Stairs', 'Standing', 'Walking']

# calculate recall for each class
recall_matrix = conf_matrix / conf_matrix.sum(axis=1, keepdims=True)

# plot recall matrix
plt.figure(figsize=(8, 6))
sns.heatmap(recall_matrix, annot=True, fmt='.2%', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title('Recall Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

pip install skl2onnx

pip install onnxruntime

from skl2onnx import convert_sklearn
from skl2onnx.common.data_types import FloatTensorType

# Create an initial type for the model
initial_type = [('float_input', FloatTensorType([None, X_train.shape[1]]))]

# Convert the scikit-learn model to ONNX format
onnx_model = convert_sklearn(model, initial_types=initial_type)

# Save the ONNX model to a file
with open("genesis.onnx", "wb") as f:
    f.write(onnx_model.SerializeToString())

!python -m onnxruntime.tools.convert_onnx_models_to_ort genesis.onnx

import onnxruntime as ort

# Load the ONNX Runtime model
ort_session = ort.InferenceSession("genesis.ort")

import numpy as np
import onnxruntime as ort

# Load the ONNX Runtime model
ort_session = ort.InferenceSession("genesis.ort")

# Convert the input data to float32
input_data = np.array(X_test, dtype=np.float32)

# Run inference
output = ort_session.run(None, {'float_input': input_data})

# Extract the predictions
predictions = output[0]
print(predictions)

report = classification_report(y_test, predictions)
print(report)

from imblearn.metrics import classification_report_imbalanced

report = classification_report_imbalanced(y_test, predictions)
print(report)